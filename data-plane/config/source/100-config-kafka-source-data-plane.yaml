# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-kafka-source-data-plane
  namespace: knative-eventing
  labels:
    app.kubernetes.io/version: devel
  annotations:
    knative.dev/example-checksum: "8157ecb1"
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # All configurations in this ConfigMap are globally applied to each
    # resource and there is no way to change them on a per-resource basis,
    # unless otherwise specified.

    # Consumer configuration are documented in https://kafka.apache.org/documentation/#consumerconfigs.
    # Some configurations might be forced by the actual code to make sure we respect the Knative Eventing
    # delivery constraints, for example, `key.deserializer` and `value.deserializer`.
    config-kafka-source-consumer.properties: |
      key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer=io.cloudevents.kafka.CloudEventDeserializer
      fetch.min.bytes=1

    # Available Vertx WebClientOptions are documented in
    # https://vertx.io/docs/apidocs/io/vertx/ext/web/client/WebClientOptions.html.
    #
    # Each egress resource (KafkaSource, Trigger, Subscription) creates an HTTP client in each pod where the resource is
    # scheduled, meaning that a client isn't shared across multiple resources to provide better isolation.
    #
    # The mapping is the following:
    #  for each method starting with `set` there is a property that can be set with the name that follows the `set`
    #  prefix starting with a lowercase letter.
    # For example, there is a method called `setIdleTimeout` and the associated property is `idleTimeout`.
    config-kafka-source-webclient.properties: |
      idleTimeout=10000
  config-kafka-source-producer.properties: |
    key.serializer=org.apache.kafka.common.serialization.StringSerializer
    value.serializer=io.cloudevents.kafka.CloudEventSerializer
    acks=all
    buffer.memory=33554432
    # compression.type=snappy
    retries=2147483647
    batch.size=16384
    client.dns.lookup=use_all_dns_ips
    connections.max.idle.ms=600000
    delivery.timeout.ms=120000
    linger.ms=0
    max.block.ms=60000
    max.request.size=1048576
    receive.buffer.bytes=-1
    request.timeout.ms=2000
    enable.idempotence=false
    max.in.flight.requests.per.connection=5
    metadata.max.age.ms=300000
    # metric.reporters=""
    metrics.num.samples=2
    metrics.recording.level=INFO
    metrics.sample.window.ms=30000
    reconnect.backoff.max.ms=1000
    reconnect.backoff.ms=50
    retry.backoff.ms=100
    # transaction.timeout.ms=60000
    # transactional.id=null
  config-kafka-source-consumer.properties: |
    cloudevent.invalid.transformer.enabled=true
    cloudevent.invalid.kind.plural=kafkasources
    key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
    value.deserializer=io.cloudevents.kafka.CloudEventDeserializer
    fetch.min.bytes=1
    heartbeat.interval.ms=3000
    max.partition.fetch.bytes=65536
    session.timeout.ms=10000
    # ssl.key.password=
    # ssl.keystore.location=
    # ssl.keystore.password=
    # ssl.truststore.location=
    # ssl.truststore.password=
    allow.auto.create.topics=true
    auto.offset.reset=earliest
    client.dns.lookup=use_all_dns_ips
    connections.max.idle.ms=540000
    default.api.timeout.ms=2000
    enable.auto.commit=false
    exclude.internal.topics=true
    fetch.max.bytes=52428800
    isolation.level=read_committed
    max.poll.interval.ms=300000
    max.poll.records=50
    partition.assignment.strategy=org.apache.kafka.clients.consumer.StickyAssignor
    receive.buffer.bytes=65536
    request.timeout.ms=2000
    # sasl.client.callback.handler.class=
    # sasl.jaas.config=
    # sasl.kerberos.service.name=
    # sasl.login.callback.handler.class
    # sasl.login.class
    # sasl.mechanism
    security.protocol=PLAINTEXT
    send.buffer.bytes=131072
    # ssl.enabled.protocols=
    # ssl.keystore.type=
    # ssl.protocol=
    # ssl.provider=
    auto.commit.interval.ms=5000
    check.crcs=true
    # client.rack=
    fetch.max.wait.ms=500
    # interceptor.classes=
    metadata.max.age.ms=600000
    # metrics.reporters=
    # metrics.num.samples=
    # metrics.recording.level=INFO
    # metrics.sample.window.ms=
    reconnect.backoff.max.ms=1000
    retry.backoff.ms=100
    # sasl.kerberos.kinit.cmd=
    # sasl.kerberos.min.time.before.relogin=
    # sasl.kerberos.ticket.renew.jitter=
    # sasl.login.refresh.buffer.seconds=
    # sasl.login.refresh.min.period.seconds=
    # sasl.login.refresh.window.factor
    # sasl.login.refresh.window.jitter
    # security.providers
    # ssl.cipher.suites
    # ssl.endpoint.identification.algorithm
    # ssl.keymanager.algorithm
    # ssl.secure.random.implementation
    # ssl.trustmanager.algorithm
  config-kafka-source-webclient.properties: |
    idleTimeout=10000
    maxPoolSize=100
